<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>KCT framework (Posts about using_kct_blog)</title><link>https://kulvait.github.io/KCT_doc/</link><description></description><atom:link href="https://kulvait.github.io/KCT_doc/categories/using_kct_blog.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2022 &lt;a href="mailto:kulvait at gmail dot com"&gt;Vojtěch Kulvait&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/a&gt;&lt;span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"&gt;KCT framework wiki and blog&lt;/span&gt; by &lt;span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"&gt;Vojtěch Kulvait&lt;/span&gt; is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License&lt;/a&gt;. &lt;br/&gt;
KCT framework itself is licensed under the terms of GNU GPL3 license. &lt;br/&gt;
Any data provided on this site or within the KCT framework may be subject to different licensing arrangements.
</copyright><lastBuildDate>Tue, 15 Mar 2022 14:41:34 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Working with KCT CBCT 5 Parallel beam geometry</title><link>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-5-parallel-beam-geometry.html</link><dc:creator>Vojtěch Kulvait</dc:creator><description>&lt;p&gt;In the &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;previous chapter&lt;/a&gt; we learned about cone beam geometry and projective matrices. For synchrotron applications might be convenient to work also with parallel ray geometry. Here we review the concept of the projection matrices to define this type of geometry.&lt;/p&gt;
&lt;h2&gt;Defining parallel rays geometry&lt;/h2&gt;
&lt;p&gt;Parallel rays geometry is simply projecting 3D points onto 2D plane. Here we review how the parallel rays geometry is encoded in other tools and if we can use the idea of projection matrices to describe it.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://www.astra-toolbox.com/docs/geom3d.html#projection-geometries"&gt;ASTRA toolbox&lt;/a&gt; parallel ray geometry in 3D is described by 12 numbers representing four 3D vectors.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;the center of the detector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_x$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_y$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additional two integers need to be supplied representing detector size&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NX&lt;/td&gt;
&lt;td&gt;X dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NY&lt;/td&gt;
&lt;td&gt;Y dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h2&gt;Defining parallel rays geometry&lt;/h2&gt;
&lt;p&gt;Parallel rays geometry is simply projecting 3D points onto 2D plane. Here we review how the parallel rays geometry is encoded in other tools and if we can use the idea of projection matrices to describe it.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://www.astra-toolbox.com/docs/geom3d.html#projection-geometries"&gt;ASTRA toolbox&lt;/a&gt; parallel ray geometry in 3D is described by 12 numbers representing four 3D vectors.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;the center of the detector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_x$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_y$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additional two integers need to be supplied representing detector size&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NX&lt;/td&gt;
&lt;td&gt;X dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NY&lt;/td&gt;
&lt;td&gt;Y dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Parallel rays geometry is simply projecting 3D points onto 2D plane. Here we review how the parallel rays geometry is encoded in other tools and if we can use the idea of projection matrices to describe it.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://www.astra-toolbox.com/docs/geom3d.html#projection-geometries"&gt;ASTRA toolbox&lt;/a&gt; parallel ray geometry in 3D is described by 12 numbers representing four 3D vectors.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;the center of the detector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_x$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_y$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additional two integers need to be supplied representing detector size&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NX&lt;/td&gt;
&lt;td&gt;X dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NY&lt;/td&gt;
&lt;td&gt;Y dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In &lt;a href="https://www.astra-toolbox.com/docs/geom3d.html#projection-geometries"&gt;ASTRA toolbox&lt;/a&gt; parallel ray geometry in 3D is described by 12 numbers representing four 3D vectors.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;the center of the detector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_x$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_y$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additional two integers need to be supplied representing detector size&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NX&lt;/td&gt;
&lt;td&gt;X dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NY&lt;/td&gt;
&lt;td&gt;Y dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;the center of the detector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_x$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$v_y$&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additional two integers need to be supplied representing detector size&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NX&lt;/td&gt;
&lt;td&gt;X dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NY&lt;/td&gt;
&lt;td&gt;Y dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Additional two integers need to be supplied representing detector size&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NX&lt;/td&gt;
&lt;td&gt;X dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NY&lt;/td&gt;
&lt;td&gt;Y dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NX&lt;/td&gt;
&lt;td&gt;X dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NY&lt;/td&gt;
&lt;td&gt;Y dimension of detector in pixel count.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;It is straightforward to see that such description is sufficient to fully describe the setup. Now we need to use this description to project point $x = (x_1, x_2, x_3)$ onto the detector by means of transform $P(x) = (PX, PY)$. For a parameter $t$ we know that the projection of all points $x + t \cdot r$ shall be the same for all $t \in \mathbb{R}$. It is natural to model $PX(x)$ and $PY(x)$ by means of affine transform&lt;/p&gt;
&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;$$PX(x) = px_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$
$$PY(x) = py_0 + b_1 x_1 + b_2 x_2 + b_3 x_3$$&lt;/p&gt;
&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;and since $P(x + t \cdot r) = P (x)$ the vectors $a$ and $b$ must be orthogonal to $r$. We also know that $PX(v_x) - PX(0) = 1$ and $PY(v_y) - PY(0) = 1$ so from other algebraic consideration we conclude that $a$ and $b$ will be multiples of orthogonalized vectors&lt;/p&gt;
&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;$$v_x^0 = v_x - (v_x, r)/(r,r) r,$$ 
$$v_y^0 = v_y - (v_y, r)/(r,r) r.$$ &lt;/p&gt;
&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In particular&lt;/p&gt;
&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;$$a = v_x^0/(v_x^0,v_x^0),$$
$$b = v_y^0/(v_y^0,v_y^0).$$&lt;/p&gt;
&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;When we project center of the detector $d$ onto the detector obviously we shall obtain center of it&lt;/p&gt;
&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;$$PX(d) = px_0 + (d, a) = 0.5 NX,$$
$$PY(d) = py_0 + (d, b) = 0.5 NY$$&lt;/p&gt;
&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;therefore &lt;/p&gt;
&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;$$px_0 = 0.5 NX - (d,a),$$
$$py_0 = 0.5 NY - (d,b).$$&lt;/p&gt;
&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Therefore the formula to get PX and PY for arbitrary point x will read&lt;/p&gt;
&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;$$PX(x) = (x, a)-(d, a)+0.5 NX,$$
$$PY(x) = (x, b)-(d, b)+0.5 NY.$$&lt;/p&gt;
&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;You can imediatelly see that we have affine transformation and that we can create $4 \times 2$ projection matrix, which for given x project it to the position on the detector. &lt;/p&gt;
&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;When instead of center of the detector we use the origin of the detector, the projection matrix will be independent of projector dimensions, therefore the classes &lt;code&gt;Geometry3DParallel&lt;/code&gt; and &lt;code&gt;Geometry3DParallelCameraMatrix&lt;/code&gt; have the constructor analogous to ASTRA, where instead of the center of the detector there is detector origin. Detector origin is a point on the detector with coordinates PX=PY=0 and by convence it is in the center of corner pixel. We use the following initialization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Vector&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;td&gt;ray direction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;td&gt;the origin of the detector, point (0,0) by convention at the center of (0,0) pixel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;u&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v&lt;/td&gt;
&lt;td&gt;the vector from detector pixel (0,0) to (1,0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;It holds that&lt;/p&gt;
&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;$$PX(o) = px_0 + (o, a) = 0,$$
$$PY(o) = py_0 + (o, b) = 0.$$&lt;/p&gt;
&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Therefore we have $px_0 = -(o,a)$, $py_0 = -(o,b)$ and
$$PX(x) = (x, a)-(o, a),$$
$$PY(x) = (x, b)-(o, b).$$&lt;/p&gt;
&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Another observation is that we are not able to recover vectors $v_x$ and $v_y$ from the projection matrix or the transformation $P$. The reason is that the matrix is based on vectors $v_x^0$ and $v_y^0$, which are orthogonal to the incomming rays. Therefore if we need the information about the tilt of the detector, we need to provide it separatelly. For all applications shall be sufficent to know the cosine of the angle between the detector and surface orthogonal to incoming rays, which is usually 1.&lt;/p&gt;
&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h2&gt;Projection matrices&lt;/h2&gt;
&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;For projection of the point x we use homogeneous coordinates, where we represent $x = (x_1, x_2, x_3, 1)$ and the 3D parallel projection matrix
$$
\mathbf{P} = \begin{pmatrix}
a_1&amp;amp;a_2&amp;amp;a_3&amp;amp;px_0 \\
b_1&amp;amp;b_2&amp;amp;b_3&amp;amp;py_0
\end{pmatrix},
$$
which represents the projection onto the detector orthogonal to the incomming rays with particular origin. It is obvious that for each tilted detector there must be one virtual orthogonal detector to the incomming rays. Let's recover what are the vectors $v_x=v_x^0$, $v_y=v_y^0$, the ray direction $r$ and origin $o$ of the virtual detector corresponding to the matrix $\mathbf{P}$.&lt;/p&gt;
&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Vector $r$ or homogeneous $(r,0)$ needs to be orthogonal to the rows of matrix $\mathbf{P}$ since $\mathbf{P} (x,1)^\top$ = $\mathbf{P} (x,1)^\top + t (r,0)^\top$ for all $t \in \mathbb{R}$. Such vector is
$$r = \frac{a \times b}{|a \times b|}.$$ &lt;/p&gt;
&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;The vectors $v_x$ and $v_y$ have the property $(v_x, a) = 1$, $(v_x,b) = 0$, $(v_x,r)=0$, $(v_y,b) = 1$, $(v_y, a) = 0$, $(v_y, r)=0$. Therefore $v_x$ and $v_y$ are scaled vectors $a$ and $b$, namely
$$v_x = \frac{a}{(a,a)},$$&lt;br&gt;
$$v_y = \frac{b}{(b,b)}.$$  &lt;/p&gt;
&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Now remains to identify vector $o$ corresponding to the origin. In fact there is a line, which projects to (0,0) so that best will be to obtain minimum norm solution, which will be a linear combination of vectors a and b, whose linear span creates a subspace of dimension two, that in turn means that there exist unique decomposition of $o = \alpha a + \beta b$ for which it holds that
$\mathbf{P} (o,1)^\top = (0,0)^\top$. For orthogonal $a$ and $b$ is the situation simplest but let's assume these vectors are not orthogonal we end up with the system
$$ \begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
$$
and the solution
$$ 
\begin{pmatrix}
\alpha \\
\beta
\end{pmatrix}
=
-\begin{pmatrix}
(a,a) &amp;amp; (a,b) \\
(b,a) &amp;amp; (b,b)
\end{pmatrix}^{-1}
\begin{pmatrix}
px_0 \\
py_0
\end{pmatrix}
.$$&lt;/p&gt;
&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Finally we conclude $o = \alpha a + \beta b$, which completes the correspondence between projective operator given as matrix and explicit declaration of the geometry in ASTRA style. Note that previous equation can be also used for establishing inverse projective operator and its minimum norm solution. Instead of (0, 0) we can use any other point on the projection plane.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;
</description><category>using_kct_blog</category><guid>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-5-parallel-beam-geometry.html</guid><pubDate>Fri, 11 Mar 2022 10:09:24 GMT</pubDate></item><item><title>Working with KCT CBCT 4 First cone beam CT projection</title><link>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-4-first-cone-beam-ct-projection.html</link><dc:creator>Vojtěch Kulvait</dc:creator><description>&lt;p&gt;In the last post Working with KCT CBCT 1 we converted DICOM data to the DEN format and explained how volumes are encoded. Today we create a CT trajectory setup and encode it in the format that can be recognized in KCT. Finally, we use KCT to reproject the volume from the last post using this trajectory.&lt;/p&gt;
&lt;h3&gt;Theoretical scanning trajectory&lt;/h3&gt;
&lt;p&gt;Let's reproject the volume we have using theoretical C-Arm FDCT trajectory. Let's say that our device rotates along the axis parallel to the z axis. By angle $\omega$ we denote polar angle between normal to detector that is pointing towards the source and x axis. Let's fix source to isocenter and source to detector lengths and create a setup in which patient is irradiated from 360 different angles corresponding to integer $\omega$ values. Let's create (camera matrices)[https://en.wikipedia.org/wiki/Camera_matrix], which encode this setup. We create them as float64 DEN files, where z dimension will represent number of angles. So that we produce file of dimensions (dimx, dimy, dimz)=(3,4,360).&lt;/p&gt;
&lt;h3&gt;Volume reprojection&lt;/h3&gt;
&lt;p&gt;Use the following command&lt;/p&gt;
&lt;pre class="code literal-block"&gt;cllin-projector --cvp --voxel-sizex 0.64453125 --voxel-sizey 0.64453125 --voxel-sizez 1.5 --projection-sizex 616 --projection-sizey 480 DEN_RAW/Series_00.den CM.den Series_00.proj
&lt;/pre&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Theoretical scanning trajectory&lt;/h3&gt;
&lt;p&gt;Let's reproject the volume we have using theoretical C-Arm FDCT trajectory. Let's say that our device rotates along the axis parallel to the z axis. By angle $\omega$ we denote polar angle between normal to detector that is pointing towards the source and x axis. Let's fix source to isocenter and source to detector lengths and create a setup in which patient is irradiated from 360 different angles corresponding to integer $\omega$ values. Let's create (camera matrices)[https://en.wikipedia.org/wiki/Camera_matrix], which encode this setup. We create them as float64 DEN files, where z dimension will represent number of angles. So that we produce file of dimensions (dimx, dimy, dimz)=(3,4,360).&lt;/p&gt;
&lt;h3&gt;Volume reprojection&lt;/h3&gt;
&lt;p&gt;Use the following command&lt;/p&gt;
&lt;pre class="code literal-block"&gt;cllin-projector --cvp --voxel-sizex 0.64453125 --voxel-sizey 0.64453125 --voxel-sizez 1.5 --projection-sizex 616 --projection-sizey 480 DEN_RAW/Series_00.den CM.den Series_00.proj
&lt;/pre&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Let's reproject the volume we have using theoretical C-Arm FDCT trajectory. Let's say that our device rotates along the axis parallel to the z axis. By angle $\omega$ we denote polar angle between normal to detector that is pointing towards the source and x axis. Let's fix source to isocenter and source to detector lengths and create a setup in which patient is irradiated from 360 different angles corresponding to integer $\omega$ values. Let's create (camera matrices)[https://en.wikipedia.org/wiki/Camera_matrix], which encode this setup. We create them as float64 DEN files, where z dimension will represent number of angles. So that we produce file of dimensions (dimx, dimy, dimz)=(3,4,360).&lt;/p&gt;
&lt;h3&gt;Volume reprojection&lt;/h3&gt;
&lt;p&gt;Use the following command&lt;/p&gt;
&lt;pre class="code literal-block"&gt;cllin-projector --cvp --voxel-sizex 0.64453125 --voxel-sizey 0.64453125 --voxel-sizez 1.5 --projection-sizex 616 --projection-sizey 480 DEN_RAW/Series_00.den CM.den Series_00.proj
&lt;/pre&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Volume reprojection&lt;/h3&gt;
&lt;p&gt;Use the following command&lt;/p&gt;
&lt;pre class="code literal-block"&gt;cllin-projector --cvp --voxel-sizex 0.64453125 --voxel-sizey 0.64453125 --voxel-sizez 1.5 --projection-sizex 616 --projection-sizey 480 DEN_RAW/Series_00.den CM.den Series_00.proj
&lt;/pre&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Use the following command&lt;/p&gt;
&lt;pre class="code literal-block"&gt;cllin-projector --cvp --voxel-sizex 0.64453125 --voxel-sizey 0.64453125 --voxel-sizez 1.5 --projection-sizex 616 --projection-sizey 480 DEN_RAW/Series_00.den CM.den Series_00.proj
&lt;/pre&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;cllin-projector --cvp --voxel-sizex 0.64453125 --voxel-sizey 0.64453125 --voxel-sizez 1.5 --projection-sizex 616 --projection-sizey 480 DEN_RAW/Series_00.den CM.den Series_00.proj
&lt;/pre&gt;&lt;/body&gt;&lt;/html&gt;
</description><category>using_kct_blog</category><guid>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-4-first-cone-beam-ct-projection.html</guid><pubDate>Sun, 19 Sep 2021 12:28:24 GMT</pubDate></item><item><title>Working with KCT CBCT 3 Python implementation of circular CT trajectory</title><link>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-3-python-implementation-of-circular-ct-trajectory.html</link><dc:creator>Vojtěch Kulvait</dc:creator><description>&lt;p&gt;In the last post we build the mathematical foundation how to construct camera matrices for FDCT setup. Now we use this knowledge to create DEN file with a circular geometry and show how to work with it in the KCT package.&lt;/p&gt;
&lt;h3&gt;System setup&lt;/h3&gt;
&lt;p&gt;We have yet mastered the theory so let's produce the set of camera matrices for given trajectory.
I have put a script that implements what has been said &lt;a href="https://github.com/kulvait/KCT_scripts/blob/master/createCameraMatricesForCircularScanTrajectory.py"&gt;to github&lt;/a&gt;.
Version of &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt; package must be at least 1.1.2 to have function for storing double DEN &lt;code&gt;storeNdarrayAsDoubleDEN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this demonstration to run on your computer you first need to install &lt;code&gt;denpy&lt;/code&gt; package and clone &lt;a href="https://github.com/kulvait/KCT_scripts"&gt;DEN scripts&lt;/a&gt; package. Let's suppose you are running Debian 11. &lt;/p&gt;
&lt;p&gt;You can create a folder &lt;code&gt;KCT&lt;/code&gt; and &lt;code&gt;KCT_bin&lt;/code&gt; in your home directory. Go into this folder and clone KCT scripts into the folder scripts.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_scripts.git scripts
&lt;/pre&gt;
&lt;p&gt;Then into your .bashrc you can add the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;System setup&lt;/h3&gt;
&lt;p&gt;We have yet mastered the theory so let's produce the set of camera matrices for given trajectory.
I have put a script that implements what has been said &lt;a href="https://github.com/kulvait/KCT_scripts/blob/master/createCameraMatricesForCircularScanTrajectory.py"&gt;to github&lt;/a&gt;.
Version of &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt; package must be at least 1.1.2 to have function for storing double DEN &lt;code&gt;storeNdarrayAsDoubleDEN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this demonstration to run on your computer you first need to install &lt;code&gt;denpy&lt;/code&gt; package and clone &lt;a href="https://github.com/kulvait/KCT_scripts"&gt;DEN scripts&lt;/a&gt; package. Let's suppose you are running Debian 11. &lt;/p&gt;
&lt;p&gt;You can create a folder &lt;code&gt;KCT&lt;/code&gt; and &lt;code&gt;KCT_bin&lt;/code&gt; in your home directory. Go into this folder and clone KCT scripts into the folder scripts.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_scripts.git scripts
&lt;/pre&gt;
&lt;p&gt;Then into your .bashrc you can add the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;We have yet mastered the theory so let's produce the set of camera matrices for given trajectory.
I have put a script that implements what has been said &lt;a href="https://github.com/kulvait/KCT_scripts/blob/master/createCameraMatricesForCircularScanTrajectory.py"&gt;to github&lt;/a&gt;.
Version of &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt; package must be at least 1.1.2 to have function for storing double DEN &lt;code&gt;storeNdarrayAsDoubleDEN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this demonstration to run on your computer you first need to install &lt;code&gt;denpy&lt;/code&gt; package and clone &lt;a href="https://github.com/kulvait/KCT_scripts"&gt;DEN scripts&lt;/a&gt; package. Let's suppose you are running Debian 11. &lt;/p&gt;
&lt;p&gt;You can create a folder &lt;code&gt;KCT&lt;/code&gt; and &lt;code&gt;KCT_bin&lt;/code&gt; in your home directory. Go into this folder and clone KCT scripts into the folder scripts.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_scripts.git scripts
&lt;/pre&gt;
&lt;p&gt;Then into your .bashrc you can add the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;For this demonstration to run on your computer you first need to install &lt;code&gt;denpy&lt;/code&gt; package and clone &lt;a href="https://github.com/kulvait/KCT_scripts"&gt;DEN scripts&lt;/a&gt; package. Let's suppose you are running Debian 11. &lt;/p&gt;
&lt;p&gt;You can create a folder &lt;code&gt;KCT&lt;/code&gt; and &lt;code&gt;KCT_bin&lt;/code&gt; in your home directory. Go into this folder and clone KCT scripts into the folder scripts.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_scripts.git scripts
&lt;/pre&gt;
&lt;p&gt;Then into your .bashrc you can add the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;You can create a folder &lt;code&gt;KCT&lt;/code&gt; and &lt;code&gt;KCT_bin&lt;/code&gt; in your home directory. Go into this folder and clone KCT scripts into the folder scripts.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_scripts.git scripts
&lt;/pre&gt;
&lt;p&gt;Then into your .bashrc you can add the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_scripts.git scripts
&lt;/pre&gt;
&lt;p&gt;Then into your .bashrc you can add the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Then into your .bashrc you can add the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=$&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_bin&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;~/&lt;/span&gt;&lt;span class="n"&gt;KCT_scripts&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Then you shall also install &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;denpy&lt;/a&gt;&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;pip3 install --user git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;if you need to upgrade run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;pip3 install --user --upgrade git+https://github.com/kulvait/KCT_denpy.git
&lt;/pre&gt;
&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Now you have installed everything to produce the camera matrices. Optionally you might need to install KCT dentk package, that will be very useful for working with the files in &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Go to the KCT folder and clone it&lt;/p&gt;
&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;git clone https://github.com/kulvait/KCT_dentk dentk
&lt;/pre&gt;
&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Then follow the instructions in README file at https://github.com/kulvait/KCT_dentk to build the package.&lt;/p&gt;
&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Python implementation&lt;/h3&gt;
&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;When the system is set up, we can just run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;createCameraMatricesForCircularScanTrajectory.py --force --write-params-file CM.den
&lt;/pre&gt;
&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;as I set all the default parameters of this script according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;last post&lt;/a&gt;. You can see that the den file &lt;code&gt;CM.den&lt;/code&gt; and the text file &lt;code&gt;CM.den.params&lt;/code&gt; were produced. Params file is not apriori needed for working with KCT but it is a good practice to know how your files were created.&lt;/p&gt;
&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Let's have a look to the main part of the script&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_isocenter&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_to_detector&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizey&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;projection_sizex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizex&lt;/span&gt;
&lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_sizey&lt;/span&gt;
&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number_of_angles&lt;/span&gt;
&lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;omega_zero&lt;/span&gt;
&lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;

&lt;span class="c1"&gt;#Let's create specified set of projection matrices as np.array&lt;/span&gt;
&lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;viewIndex&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VIEWCOUNT&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sourcePosition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pixel_offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;A2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_A1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;_X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OMEGA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_A3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_A1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_E&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;CameraMatrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;OMEGA&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;OMEGAINCREMENT&lt;/span&gt;

&lt;span class="n"&gt;DEN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storeNdarrayAsDoubleDEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMatrixFile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CameraMatrices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ARG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;First we initialize the parameters.&lt;/p&gt;
&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Then we prepare the numpy matrices according to &lt;a href="https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html"&gt;the last post&lt;/a&gt; and finally we do the multiplication and concatenate the matrices.&lt;/p&gt;
&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;On the last line you can see how easy is to convert &lt;code&gt;numpy.ndarray&lt;/code&gt; into the DEN by one call. &lt;/p&gt;
&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Using the script, we produce the camera matrices we need for given trajectory setup.
To remind you the parameters let's have a look to the &lt;code&gt;CM.den.params&lt;/code&gt; file&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"_json_message"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"Created using KCT script createCameraMatricesForCircularScanTrajectory.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"force"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"number_of_angles"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;360&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"omega_zero"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"outputMatrixFile"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"CM.den"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsetx"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_offsety"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"pixel_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizex"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;616&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"projection_sizey"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_detector"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1198.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"source_to_isocenter"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;749.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;"write_params_file"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;So that we can check which particular parameters were used to create our trajectory. To dig deeper, I have written &lt;code&gt;dentk-mathinfo&lt;/code&gt; as part of the dentk package. We can check decompose our matrices and check properties such as source position of first few frames by running &lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;dentk-matinfo CM.den -f &lt;span class="m"&gt;0&lt;/span&gt;-5
&lt;/pre&gt;
&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;with the following output&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;Camera matrix from 0-th frame:
    |   -0.200     1.623     0.000   149.737|                     | 1944.805    -0.000   239.500| |    0.000     1.000     0.000|    0.000|
P = |   -0.257     0.000    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001     0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000     0.000    -0.000|  749.000|
S = [749.00, -0.00, -0.00], -Q^T u = [749.00,  0.00,  0.00].
Camera matrix from 1-th frame:
    |   -0.228     1.620     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.017     1.000    -0.000|    0.000|
P = |   -0.257    -0.004    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000    -0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -1.000    -0.017    -0.000|  749.000|
S = [748.89, 13.07, -0.00], -Q^T u = [748.89, 13.07,  0.00].
Camera matrix from 2-th frame:
    |   -0.256     1.615     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.035     0.999    -0.000|    0.000|
P = |   -0.257    -0.009    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.035    -0.000|  749.000|
S = [748.54, 26.14, -0.00], -Q^T u = [748.54, 26.14,  0.00].
Camera matrix from 3-th frame:
    |   -0.285     1.611     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.052     0.999    -0.000|    0.000|
P = |   -0.256    -0.013    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|   -0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.999    -0.052    -0.000|  749.000|
S = [747.97, 39.20, -0.00], -Q^T u = [747.97, 39.20,  0.00].
Camera matrix from 4-th frame:
    |   -0.313     1.605     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.070     0.998    -0.000|   -0.000|
P = |   -0.256    -0.018    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.998    -0.070    -0.000|  749.000|
S = [747.18, 52.25, -0.00], -Q^T u = [747.18, 52.25,  0.00].
Camera matrix from 5-th frame:
    |   -0.341     1.600     0.000   149.737|                     | 1944.805     0.000   239.500| |   -0.087     0.996     0.000|   -0.000|
P = |   -0.256    -0.022    -1.623   192.252| = C[Q|u] =    1198.0|    0.000  1944.805   307.500|.|    0.000     0.000    -1.000|    0.000|
    |   -0.001    -0.000     0.000     0.625|                     |    0.000     0.000     1.000| |   -0.996    -0.087    -0.000|  749.000|
S = [746.15, 65.28, -0.00], -Q^T u = [746.15, 65.28,  0.00].
&lt;/pre&gt;
&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Here we can see that it was among other things able to show us source positions $S$ for the first views. This seems reasonable when looking to the image of the geometry and how we describe the trajectory. First the source is aligned with $x_1$ axis and it rotates towards $x_2$ axis.&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In the next post we use created camera matrices, KCT cbct package and downloaded CT volume from public repository and we will show how to project the CT volumes using the FDCT trajectory, which we just created.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;
</description><category>using_kct_blog</category><guid>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-3-python-implementation-of-circular-ct-trajectory.html</guid><pubDate>Wed, 15 Sep 2021 10:05:50 GMT</pubDate></item><item><title>Working with KCT CBCT 2 Projective geometry and camera matrices to describe CT geometry</title><link>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html</link><dc:creator>Vojtěch Kulvait</dc:creator><description>&lt;p&gt;Before we define particular geometry corresponding to the flat panel detector CT trajectory, we need to know some theory about projective geometry and camera matrices. This will be the content of this post.&lt;/p&gt;
&lt;h3&gt;CT projections geometry&lt;/h3&gt;
&lt;p&gt;In the computer tomography, we project 3D object in $ \mathbb{R}^3 $ to the projector grid. Let's say it is a two dimensional grid that consist of rectangular pixels. Coordinates on the detector can be naturally described as the vectors in $ \mathbb{R}^2 $ since the projections are 2D images. The process of the X-ray projection is analogous to the pinhole camera model that projects the 3D scene onto the 2D plane. And therefore projective geometry is a good tool to study this correspondence.&lt;/p&gt;
&lt;h3&gt;Projective geometry&lt;/h3&gt;
&lt;p&gt;Projective space is a structure on top of an Vector space $\mathbf{V}$ that is not a vector space itself. For an introduction into the projective geometry, see &lt;a href="http://math.unife.it/insegnamenti/geometria-differenziale/materiale-didattico/projective.pdf"&gt;class notes of Nigel Hitchin&lt;/a&gt;. I will follow some of its definitions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The projective space $\mathcal{P}(\mathbf{V})$ of a vector space $\mathbf{V}$ is a set of one dimensional subspaces of $\mathbf{V}$. The dimension of $\mathcal{P}(\mathbf{V})$ is $dim(\mathbf{V}) - 1$. Projective space of dimension $1$ is called projective line and projective space of dimension $2$ is called projective plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is interesting to see in this definitions, that when we take the space $\mathbb{R}^3$, where the source is placed in its origin. We construct "a set of one dimensional subspaces of $\mathbb{R}^3$", which are all the lines through the origin representing all the rays going from the source. The number of lines with this characteristic is (almost) the same as the number of the points on the unit half sphere and I use this property, for derivation of the Cutting voxel projector. Here we first observe what uniquelly defines the flat detector CT (FDCT) setup and which properties can be described using projection matrices. &lt;/p&gt;
&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;CT projections geometry&lt;/h3&gt;
&lt;p&gt;In the computer tomography, we project 3D object in $ \mathbb{R}^3 $ to the projector grid. Let's say it is a two dimensional grid that consist of rectangular pixels. Coordinates on the detector can be naturally described as the vectors in $ \mathbb{R}^2 $ since the projections are 2D images. The process of the X-ray projection is analogous to the pinhole camera model that projects the 3D scene onto the 2D plane. And therefore projective geometry is a good tool to study this correspondence.&lt;/p&gt;
&lt;h3&gt;Projective geometry&lt;/h3&gt;
&lt;p&gt;Projective space is a structure on top of an Vector space $\mathbf{V}$ that is not a vector space itself. For an introduction into the projective geometry, see &lt;a href="http://math.unife.it/insegnamenti/geometria-differenziale/materiale-didattico/projective.pdf"&gt;class notes of Nigel Hitchin&lt;/a&gt;. I will follow some of its definitions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The projective space $\mathcal{P}(\mathbf{V})$ of a vector space $\mathbf{V}$ is a set of one dimensional subspaces of $\mathbf{V}$. The dimension of $\mathcal{P}(\mathbf{V})$ is $dim(\mathbf{V}) - 1$. Projective space of dimension $1$ is called projective line and projective space of dimension $2$ is called projective plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is interesting to see in this definitions, that when we take the space $\mathbb{R}^3$, where the source is placed in its origin. We construct "a set of one dimensional subspaces of $\mathbb{R}^3$", which are all the lines through the origin representing all the rays going from the source. The number of lines with this characteristic is (almost) the same as the number of the points on the unit half sphere and I use this property, for derivation of the Cutting voxel projector. Here we first observe what uniquelly defines the flat detector CT (FDCT) setup and which properties can be described using projection matrices. &lt;/p&gt;
&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In the computer tomography, we project 3D object in $ \mathbb{R}^3 $ to the projector grid. Let's say it is a two dimensional grid that consist of rectangular pixels. Coordinates on the detector can be naturally described as the vectors in $ \mathbb{R}^2 $ since the projections are 2D images. The process of the X-ray projection is analogous to the pinhole camera model that projects the 3D scene onto the 2D plane. And therefore projective geometry is a good tool to study this correspondence.&lt;/p&gt;
&lt;h3&gt;Projective geometry&lt;/h3&gt;
&lt;p&gt;Projective space is a structure on top of an Vector space $\mathbf{V}$ that is not a vector space itself. For an introduction into the projective geometry, see &lt;a href="http://math.unife.it/insegnamenti/geometria-differenziale/materiale-didattico/projective.pdf"&gt;class notes of Nigel Hitchin&lt;/a&gt;. I will follow some of its definitions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The projective space $\mathcal{P}(\mathbf{V})$ of a vector space $\mathbf{V}$ is a set of one dimensional subspaces of $\mathbf{V}$. The dimension of $\mathcal{P}(\mathbf{V})$ is $dim(\mathbf{V}) - 1$. Projective space of dimension $1$ is called projective line and projective space of dimension $2$ is called projective plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is interesting to see in this definitions, that when we take the space $\mathbb{R}^3$, where the source is placed in its origin. We construct "a set of one dimensional subspaces of $\mathbb{R}^3$", which are all the lines through the origin representing all the rays going from the source. The number of lines with this characteristic is (almost) the same as the number of the points on the unit half sphere and I use this property, for derivation of the Cutting voxel projector. Here we first observe what uniquelly defines the flat detector CT (FDCT) setup and which properties can be described using projection matrices. &lt;/p&gt;
&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Projective geometry&lt;/h3&gt;
&lt;p&gt;Projective space is a structure on top of an Vector space $\mathbf{V}$ that is not a vector space itself. For an introduction into the projective geometry, see &lt;a href="http://math.unife.it/insegnamenti/geometria-differenziale/materiale-didattico/projective.pdf"&gt;class notes of Nigel Hitchin&lt;/a&gt;. I will follow some of its definitions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The projective space $\mathcal{P}(\mathbf{V})$ of a vector space $\mathbf{V}$ is a set of one dimensional subspaces of $\mathbf{V}$. The dimension of $\mathcal{P}(\mathbf{V})$ is $dim(\mathbf{V}) - 1$. Projective space of dimension $1$ is called projective line and projective space of dimension $2$ is called projective plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is interesting to see in this definitions, that when we take the space $\mathbb{R}^3$, where the source is placed in its origin. We construct "a set of one dimensional subspaces of $\mathbb{R}^3$", which are all the lines through the origin representing all the rays going from the source. The number of lines with this characteristic is (almost) the same as the number of the points on the unit half sphere and I use this property, for derivation of the Cutting voxel projector. Here we first observe what uniquelly defines the flat detector CT (FDCT) setup and which properties can be described using projection matrices. &lt;/p&gt;
&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Projective space is a structure on top of an Vector space $\mathbf{V}$ that is not a vector space itself. For an introduction into the projective geometry, see &lt;a href="http://math.unife.it/insegnamenti/geometria-differenziale/materiale-didattico/projective.pdf"&gt;class notes of Nigel Hitchin&lt;/a&gt;. I will follow some of its definitions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The projective space $\mathcal{P}(\mathbf{V})$ of a vector space $\mathbf{V}$ is a set of one dimensional subspaces of $\mathbf{V}$. The dimension of $\mathcal{P}(\mathbf{V})$ is $dim(\mathbf{V}) - 1$. Projective space of dimension $1$ is called projective line and projective space of dimension $2$ is called projective plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is interesting to see in this definitions, that when we take the space $\mathbb{R}^3$, where the source is placed in its origin. We construct "a set of one dimensional subspaces of $\mathbb{R}^3$", which are all the lines through the origin representing all the rays going from the source. The number of lines with this characteristic is (almost) the same as the number of the points on the unit half sphere and I use this property, for derivation of the Cutting voxel projector. Here we first observe what uniquelly defines the flat detector CT (FDCT) setup and which properties can be described using projection matrices. &lt;/p&gt;
&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;blockquote&gt;
&lt;p&gt;The projective space $\mathcal{P}(\mathbf{V})$ of a vector space $\mathbf{V}$ is a set of one dimensional subspaces of $\mathbf{V}$. The dimension of $\mathcal{P}(\mathbf{V})$ is $dim(\mathbf{V}) - 1$. Projective space of dimension $1$ is called projective line and projective space of dimension $2$ is called projective plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is interesting to see in this definitions, that when we take the space $\mathbb{R}^3$, where the source is placed in its origin. We construct "a set of one dimensional subspaces of $\mathbb{R}^3$", which are all the lines through the origin representing all the rays going from the source. The number of lines with this characteristic is (almost) the same as the number of the points on the unit half sphere and I use this property, for derivation of the Cutting voxel projector. Here we first observe what uniquelly defines the flat detector CT (FDCT) setup and which properties can be described using projection matrices. &lt;/p&gt;
&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;It is interesting to see in this definitions, that when we take the space $\mathbb{R}^3$, where the source is placed in its origin. We construct "a set of one dimensional subspaces of $\mathbb{R}^3$", which are all the lines through the origin representing all the rays going from the source. The number of lines with this characteristic is (almost) the same as the number of the points on the unit half sphere and I use this property, for derivation of the Cutting voxel projector. Here we first observe what uniquelly defines the flat detector CT (FDCT) setup and which properties can be described using projection matrices. &lt;/p&gt;
&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;FDCT projection setup&lt;/h3&gt;
&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Let's have the following FDCT setup&lt;/p&gt;
&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;&lt;img src="https://kulvait.github.io/KCT_doc/images/FDCTProjectionGeometry.png"&gt;&lt;/p&gt;
&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;There are world coordinates described by vectors $\mathbb{x} = (x_1, x_2, x_3)$. There is a source at the position $\mathbb{S} = (s_1, s_2, s_3)$. And there is a Flat panel detector which is described by the point $\chi^{(0,0)}$, where is the point $(0,0)$ at the detector and by two orthogonal vectors $\chi^1$ and $\chi^2$. Let's say that spacing of the detector pixels is determined by the length of the vectors $\chi^1$ and $\chi^2$ so that the size of the pixels is $|\chi^1| \times |\chi^2|$. Let's also say that pixel boundaries have zero thickness and where one pixel ends, another starts. We have to specify how many pixels is there in the directions $\chi^1$ and  $\chi^2$ and we have complete FDCT setup.&lt;/p&gt;
&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;By convention described also in previous post, the $x_3$ axis parallel with the axis of the rotation and $x_2$ axis goes from the top (above scanned subject) to bottom (under scanned object).&lt;/p&gt;
&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Let's mention there is one special ray from the source, which is perpendicular to the detector. We usually call this ray principal ray and the point, where this ray hits the detector principal point. In some applications it might be convenient to shift the principal ray outside the center of the detector, see for example work on &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h4&gt;Common FDCT setup simplifications&lt;/h4&gt;
&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;The setup described is too general for many projectors implemented in KCT package. The only exception is &lt;a href="https://doi.org/10.1118/1.595715"&gt;Siddon projector&lt;/a&gt;, which can be used with the geometries of this generality.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;&lt;a href="https://doi.org/10.1109/TMI.2010.2050898"&gt;TR and TT projectors&lt;/a&gt; by design and CVP projector by current implementation use the following simplification of the geometry. They expect, that the vector $\mathbb{\chi^2}$ is always parallel to $\mathbb{x_3}$ cartesian vector. That implies then that the normal to the detector, which can be obtained e.g. as a normalized cross product $\mathbb{\chi^1} \times \mathbb{\chi^2}$ is orthogonal to $\mathbb{x_3}$. &lt;/p&gt;
&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;This exclude trajectories, where the device rotates along axis not parallel with $\mathbb{x_3}$.&lt;/p&gt;
&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h4&gt;Camera matrix to describe FDCT setup&lt;/h4&gt;
&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;By the camera matrix we describe the mapping from the point $\mathbb{x} = (x_1, x_2, x_3)$ to the point on the detector given by $\mathbb{P}=(p^1, p^2)$. The point $\mathbb{x}$ will be projected onto the point $\chi^{(0,0)} + p^1 * \chi^1 + p^2 * \chi^2$. Interestingly, we don't need to know precise values of vectors $\chi^{(0,0)}, \chi^1, \chi^2$ as when we place the vector $\chi^{(0,0)}$ anywhere on the ray from the source to that vector, and scale accordingly also the vectors $\chi^1$ and $\chi^2$, we obtain obtain the same mapping between $\mathbb{x} = (x_1, x_2, x_3)$ and $\mathbb{P}=(P^1, P^2)$. In the normal detector we can not change size of the pixels, so this is just a theoretical consideration. However this has an implication that when having camera matrix in the sense of beeing a linear mapping between projective space of dimension 3 and projective space of dimension 2, we can not tell how far from the source the detector is or how far the pixels are spaced from each other. When to the camera metrix adding information of one pixel dimension or distance from projector to source, system is fully defined. &lt;/p&gt;
&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Why talking about abstract projective elements? Camera matrix is at the end of the day matrix from $\mathbf{R}^{3x4}$. Scaling by nonzero constant do not change its properties as a projective element. Therefore using this fact we can encode source to detector distance into the matrix. On the other hand we don't need this information in order to make CT projection or reconstruction anyway. Camera matrix does not tell us, what is the size of the detector and this information must be provided to the reconstruction software by other means. For example to do a projection using KCT framework, we need to specify&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;    --projection-sizex UINT Needs: --projection-sizey
                                X dimension of detector in pixel count, defaults to 616.
    --projection-sizey UINT Needs: --projection-sizex
                                Y dimension of detector in pixel count, defaults to 480.
&lt;/pre&gt;
&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h4&gt;Creating camera matrices for particular setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Camera_matrix"&gt;Camera matrix&lt;/a&gt; is relatively well described on Wikipedia. So let's construct set of camera matrices to be used in the KCT framework. To store them, we use &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt; format of the size float64 with $(dimx, dimy, dimz)=(4,3,n)$, where n is number of the configurations in given FDCT trajectory.&lt;/p&gt;
&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Let's have the following setup. Zero of the world coordinates coincide with the volume center. Source and detector rotate along $x_3$ axis and $\chi_2$ is parallel with $x_3$. Principal ray at every position of the trajectory hits the detector. The  distance  from  the source  to  theisocenter  is $I = 749mm$ and  the  distance  from  source to the detector is $A=1198mm$. Trajectory consist of $360$ views and in the view $\omega$ the normal to the detector pointing towards the source forms the polar angle with respect to $x_1$ and $x_2$ axes so that $n_\omega = (cos(\omega), sin(\omega))$. Finally let's have $PX\times PY$ = $0.616mmx0.616mm$ pixels and $M\times N = 616x480$ grid.&lt;/p&gt;
&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;From this information we construct camera matrix for given projection setup. First we perform a transformation to the local coordinate system related to the given view $\omega$ that will be more convenient to work with. In the terminology of projection matrices, we understand the point in 3D as a point of projective space of dimension 3, which can be represented by the vector $(x_1, x_2, x_3, 1)$. We use this just as a vehicle to encode source position into the projection matrix not to exploit some topological properties of $\mathbb{R}^4$, but let's respect this usual way how to construct projection matrices. So that linear transformations in this kind of space will be transformations between $\mathbb{R}^4$ encoded by means of matrices from $\mathbb{R}^{4\times4}$.&lt;/p&gt;
&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;First we just perform a rotation of the axes $x_1,x_2,x_3$ so that $x_1',x_2',x_3'$ will again form Cartesian coordinate system but rotated towards our setup. Let's identify positive direction of $x_3'$ with positive direction of $\chi^2$ by $x_3'=-x_3$. We rotate remaining two axes in the way that $x_2'$ unit vector will be the normal vector $n_\omega$ and $x_1'$ will be colinear with $\chi^1$ on the detector. So we use the following projective element 
$$
\mathbf{X}_1 =
\begin{pmatrix} 
-\sin{\omega}&amp;amp; \cos{\omega} &amp;amp;0 &amp;amp;0 \\
\cos{\omega}&amp;amp; \sin{\omega}&amp;amp;0 &amp;amp;0 \\ 
0&amp;amp;0&amp;amp;-1&amp;amp;0 \\
0&amp;amp;0&amp;amp;0&amp;amp;1 
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Now the $\mathbf{X}_1$ took us into a new coordinate system, let's live in it and shift its origin to $S'$. First let's relize that coordinates of $S' = (0, \sqrt{s_1^2+s_2^2}, -s_3, 1)$. Now we can do a shift by means of the next projective element
$$
\mathbf{X}_2 = \begin{pmatrix} 
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;1&amp;amp;0&amp;amp;-\sqrt{s_1^2+s_2^2}\\
0&amp;amp;0&amp;amp;1&amp;amp;s_3\\
0&amp;amp;0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;So we constructed projective element such that
$$
\begin{pmatrix} x_1'' \\ x_2''\\x_3''\\1 \end{pmatrix} = \mathbf{X}_2 \mathbf{X}_1 \begin{pmatrix} x_1\\x_2\\x_3\\1 \end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Now let's construct the projective element from 3D transformed space to the detector. But simpler detector with the same orientation as our flat panel but with the zero positioned at principal point and moreover detector such, that unit vector $x_3''$ from principal point projects to 1 in $\chi^2$ direction and unit vector $x_2''$ project to 1 in $\chi_1$ direction. This detector must have focal length $A$ so that we can construct the following projective element
$$
\mathbf{E} =
\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1&amp;amp;0\\
0&amp;amp;-\frac{1}{A}&amp;amp;0&amp;amp;0
\end{pmatrix}
$$
Notice sign to correct for the fact that $x_2''$ points from detector to source and not vice versa. We know, that the unit vectors project to the distance $1/PX$ or $1/PY$ respectivelly. Let's add another projective element to correct for the pixel sizes
$$
\mathbf{A}_1 =
\begin{pmatrix}
\frac{1}{PX}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{PY}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Here is obvious that the operation of cutting pixels into $kxk$ subpixels can be realized by matrix
$$
\begin{pmatrix}
k&amp;amp;0&amp;amp;0\\
0&amp;amp;k&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
$$
and splitting is realized by inverse operation
$$
\begin{pmatrix}
\frac{1}{k}&amp;amp;0&amp;amp;0\\
0&amp;amp;\frac{1}{k}&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}.
$$&lt;/p&gt;
&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;To shift the $\chi^{(0,0)}$ to its intended position let's do this
\begin{equation}
\mathbf{A}_2 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;\frac{N-1}{2}\\
0&amp;amp;1&amp;amp;\frac{M-1}{2}\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In the KCT we use the convention that integer projection coordinates denote center of given pixel.
Therefore zero is achieved at the center of corner pixel.
Now the principal point has coordinates $(PX,PY)=((N-1)/2, (M-1)/2)$ we may need to shift it further. We might want to do something fancy as &lt;a href="https://doi.org/10.1109/TMI.2004.826950"&gt;quarter detector offset shifting to improve resolution&lt;/a&gt;. Or we might just want to adjust the positioning to the real device we are trying to model. &lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}_3 =
\begin{pmatrix}
1&amp;amp;0&amp;amp;PX_o\\
0&amp;amp;1&amp;amp;PY_o\\
0&amp;amp;0&amp;amp;1
\end{pmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Is a projective element, which does this final offsetting with $(PX_o,PY_o)=(0,0)$ in our demonstration example. &lt;/p&gt;
&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Finally we obtain camera matrix, sometimes in this context also called projection matrix, that for given position in the volume provides the position on the projector
$$
\mathbf{P} = \mathbf{A}_3 \mathbf{A}_2 \mathbf{A}_1 \mathbf{E} \mathbf{X}_2 \mathbf{X}_1.
$$&lt;/p&gt;
&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Next post&lt;/h3&gt;
&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In the next post we use our newly gained knowledge to create a Python implementation to write circular trajectory into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN file&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;
</description><category>using_kct_blog</category><guid>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-2-projective-geometry-and-camera-matrices-to-describe-ct-geometry.html</guid><pubDate>Tue, 14 Sep 2021 19:05:50 GMT</pubDate></item><item><title>Working with KCT CBCT 1 Converting DICOM volume to DEN volume</title><link>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-1-converting-dicom-volume-to-den-volume-using-dicom-data-from-public-repository.html</link><dc:creator>Vojtěch Kulvait</dc:creator><description>&lt;p&gt;In this series of blog posts, I will describe how to work with the KCT framework from the first steps of importing data to the more advanced topics as a running reconstruction. &lt;/p&gt;
&lt;p&gt;In the post today, I explain how to import files in DICOM into the DEN format that is used within the framework. I use the data from the public repository.&lt;/p&gt;
&lt;h3&gt;Example dataset&lt;/h3&gt;
&lt;p&gt;I have choosen a publicly available dataset from &lt;a href="https://public.cancerimagingarchive.net/"&gt;The Cancer Imaging Archive&lt;/a&gt; containing a brain CT scan, ID TCGA-19-1787. To download this dataset I followed the procedure described on &lt;a href="https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+TCIA+Images"&gt;The Cancer Imaging Archive page&lt;/a&gt;. It is needed to instal their application NBIA Data Retriever and provide it with the information in the &lt;code&gt;manifest-1631446347856.tcia&lt;/code&gt; file, which must contain the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;downloadServerUrl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;public&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cancerimagingarchive&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nbia&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;servlet&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;DownloadServlet&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;includeAnnotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;noOfrRetry&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;databasketId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1631446347856.&lt;/span&gt;&lt;span class="n"&gt;tcia&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;manifestVersion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ListOfSeriesToDownload&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="mf"&gt;1.3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;6.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;4.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;14519.5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;2.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;5826.4001&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;312669389023517091391958251391&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Example dataset is published under the terms of &lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons Attribution 3.0 Unported License&lt;/a&gt; and the &lt;a href="https://kulvait.github.io/KCT_doc/license-tcga-gbm.html"&gt;following terms&lt;/a&gt;. This licencing apply to any user even the user of the derived data, which I publish here in scope of this demonstration in &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we unpack the data of example volume, we see that dicom files with the suffix dcm are in the directory &lt;code&gt;TCGA-19-1787/04-20-2002-NA-NR CT HEAD W CE-97133/2.000000-Head Vol.  1.5  H40s ST-51391&lt;/code&gt;. For simplicity I rename this directory just to DICOM and remove parent subdirectories. 
From the DICOM headers we can see that it is a scan on SIEMENS Sensation Cardiac 64 CT machine. It contains $124$ slices with the thickness of $1.5$mm of the dimension $512 \times 512$, the voxel sizes are ($0.64453125$mm, $0.64453125$mm, $1.5$mm).&lt;/p&gt;
&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In the post today, I explain how to import files in DICOM into the DEN format that is used within the framework. I use the data from the public repository.&lt;/p&gt;
&lt;h3&gt;Example dataset&lt;/h3&gt;
&lt;p&gt;I have choosen a publicly available dataset from &lt;a href="https://public.cancerimagingarchive.net/"&gt;The Cancer Imaging Archive&lt;/a&gt; containing a brain CT scan, ID TCGA-19-1787. To download this dataset I followed the procedure described on &lt;a href="https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+TCIA+Images"&gt;The Cancer Imaging Archive page&lt;/a&gt;. It is needed to instal their application NBIA Data Retriever and provide it with the information in the &lt;code&gt;manifest-1631446347856.tcia&lt;/code&gt; file, which must contain the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;downloadServerUrl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;public&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cancerimagingarchive&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nbia&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;servlet&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;DownloadServlet&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;includeAnnotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;noOfrRetry&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;databasketId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1631446347856.&lt;/span&gt;&lt;span class="n"&gt;tcia&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;manifestVersion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ListOfSeriesToDownload&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="mf"&gt;1.3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;6.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;4.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;14519.5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;2.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;5826.4001&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;312669389023517091391958251391&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Example dataset is published under the terms of &lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons Attribution 3.0 Unported License&lt;/a&gt; and the &lt;a href="https://kulvait.github.io/KCT_doc/license-tcga-gbm.html"&gt;following terms&lt;/a&gt;. This licencing apply to any user even the user of the derived data, which I publish here in scope of this demonstration in &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we unpack the data of example volume, we see that dicom files with the suffix dcm are in the directory &lt;code&gt;TCGA-19-1787/04-20-2002-NA-NR CT HEAD W CE-97133/2.000000-Head Vol.  1.5  H40s ST-51391&lt;/code&gt;. For simplicity I rename this directory just to DICOM and remove parent subdirectories. 
From the DICOM headers we can see that it is a scan on SIEMENS Sensation Cardiac 64 CT machine. It contains $124$ slices with the thickness of $1.5$mm of the dimension $512 \times 512$, the voxel sizes are ($0.64453125$mm, $0.64453125$mm, $1.5$mm).&lt;/p&gt;
&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Example dataset&lt;/h3&gt;
&lt;p&gt;I have choosen a publicly available dataset from &lt;a href="https://public.cancerimagingarchive.net/"&gt;The Cancer Imaging Archive&lt;/a&gt; containing a brain CT scan, ID TCGA-19-1787. To download this dataset I followed the procedure described on &lt;a href="https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+TCIA+Images"&gt;The Cancer Imaging Archive page&lt;/a&gt;. It is needed to instal their application NBIA Data Retriever and provide it with the information in the &lt;code&gt;manifest-1631446347856.tcia&lt;/code&gt; file, which must contain the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;downloadServerUrl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;public&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cancerimagingarchive&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nbia&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;servlet&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;DownloadServlet&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;includeAnnotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;noOfrRetry&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;databasketId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1631446347856.&lt;/span&gt;&lt;span class="n"&gt;tcia&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;manifestVersion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ListOfSeriesToDownload&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="mf"&gt;1.3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;6.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;4.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;14519.5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;2.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;5826.4001&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;312669389023517091391958251391&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Example dataset is published under the terms of &lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons Attribution 3.0 Unported License&lt;/a&gt; and the &lt;a href="https://kulvait.github.io/KCT_doc/license-tcga-gbm.html"&gt;following terms&lt;/a&gt;. This licencing apply to any user even the user of the derived data, which I publish here in scope of this demonstration in &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we unpack the data of example volume, we see that dicom files with the suffix dcm are in the directory &lt;code&gt;TCGA-19-1787/04-20-2002-NA-NR CT HEAD W CE-97133/2.000000-Head Vol.  1.5  H40s ST-51391&lt;/code&gt;. For simplicity I rename this directory just to DICOM and remove parent subdirectories. 
From the DICOM headers we can see that it is a scan on SIEMENS Sensation Cardiac 64 CT machine. It contains $124$ slices with the thickness of $1.5$mm of the dimension $512 \times 512$, the voxel sizes are ($0.64453125$mm, $0.64453125$mm, $1.5$mm).&lt;/p&gt;
&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;I have choosen a publicly available dataset from &lt;a href="https://public.cancerimagingarchive.net/"&gt;The Cancer Imaging Archive&lt;/a&gt; containing a brain CT scan, ID TCGA-19-1787. To download this dataset I followed the procedure described on &lt;a href="https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+TCIA+Images"&gt;The Cancer Imaging Archive page&lt;/a&gt;. It is needed to instal their application NBIA Data Retriever and provide it with the information in the &lt;code&gt;manifest-1631446347856.tcia&lt;/code&gt; file, which must contain the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;downloadServerUrl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;public&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cancerimagingarchive&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nbia&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;servlet&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;DownloadServlet&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;includeAnnotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;noOfrRetry&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;databasketId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1631446347856.&lt;/span&gt;&lt;span class="n"&gt;tcia&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;manifestVersion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ListOfSeriesToDownload&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="mf"&gt;1.3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;6.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;4.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;14519.5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;2.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;5826.4001&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;312669389023517091391958251391&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Example dataset is published under the terms of &lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons Attribution 3.0 Unported License&lt;/a&gt; and the &lt;a href="https://kulvait.github.io/KCT_doc/license-tcga-gbm.html"&gt;following terms&lt;/a&gt;. This licencing apply to any user even the user of the derived data, which I publish here in scope of this demonstration in &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we unpack the data of example volume, we see that dicom files with the suffix dcm are in the directory &lt;code&gt;TCGA-19-1787/04-20-2002-NA-NR CT HEAD W CE-97133/2.000000-Head Vol.  1.5  H40s ST-51391&lt;/code&gt;. For simplicity I rename this directory just to DICOM and remove parent subdirectories. 
From the DICOM headers we can see that it is a scan on SIEMENS Sensation Cardiac 64 CT machine. It contains $124$ slices with the thickness of $1.5$mm of the dimension $512 \times 512$, the voxel sizes are ($0.64453125$mm, $0.64453125$mm, $1.5$mm).&lt;/p&gt;
&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;downloadServerUrl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;public&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cancerimagingarchive&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nbia&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;servlet&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;DownloadServlet&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;includeAnnotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;true&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;noOfrRetry&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;databasketId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1631446347856.&lt;/span&gt;&lt;span class="n"&gt;tcia&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;manifestVersion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ListOfSeriesToDownload&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="mf"&gt;1.3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;6.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;4.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;14519.5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;2.1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;5826.4001&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;312669389023517091391958251391&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Example dataset is published under the terms of &lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons Attribution 3.0 Unported License&lt;/a&gt; and the &lt;a href="https://kulvait.github.io/KCT_doc/license-tcga-gbm.html"&gt;following terms&lt;/a&gt;. This licencing apply to any user even the user of the derived data, which I publish here in scope of this demonstration in &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we unpack the data of example volume, we see that dicom files with the suffix dcm are in the directory &lt;code&gt;TCGA-19-1787/04-20-2002-NA-NR CT HEAD W CE-97133/2.000000-Head Vol.  1.5  H40s ST-51391&lt;/code&gt;. For simplicity I rename this directory just to DICOM and remove parent subdirectories. 
From the DICOM headers we can see that it is a scan on SIEMENS Sensation Cardiac 64 CT machine. It contains $124$ slices with the thickness of $1.5$mm of the dimension $512 \times 512$, the voxel sizes are ($0.64453125$mm, $0.64453125$mm, $1.5$mm).&lt;/p&gt;
&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Example dataset is published under the terms of &lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons Attribution 3.0 Unported License&lt;/a&gt; and the &lt;a href="https://kulvait.github.io/KCT_doc/license-tcga-gbm.html"&gt;following terms&lt;/a&gt;. This licencing apply to any user even the user of the derived data, which I publish here in scope of this demonstration in &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we unpack the data of example volume, we see that dicom files with the suffix dcm are in the directory &lt;code&gt;TCGA-19-1787/04-20-2002-NA-NR CT HEAD W CE-97133/2.000000-Head Vol.  1.5  H40s ST-51391&lt;/code&gt;. For simplicity I rename this directory just to DICOM and remove parent subdirectories. 
From the DICOM headers we can see that it is a scan on SIEMENS Sensation Cardiac 64 CT machine. It contains $124$ slices with the thickness of $1.5$mm of the dimension $512 \times 512$, the voxel sizes are ($0.64453125$mm, $0.64453125$mm, $1.5$mm).&lt;/p&gt;
&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;When we unpack the data of example volume, we see that dicom files with the suffix dcm are in the directory &lt;code&gt;TCGA-19-1787/04-20-2002-NA-NR CT HEAD W CE-97133/2.000000-Head Vol.  1.5  H40s ST-51391&lt;/code&gt;. For simplicity I rename this directory just to DICOM and remove parent subdirectories. 
From the DICOM headers we can see that it is a scan on SIEMENS Sensation Cardiac 64 CT machine. It contains $124$ slices with the thickness of $1.5$mm of the dimension $512 \times 512$, the voxel sizes are ($0.64453125$mm, $0.64453125$mm, $1.5$mm).&lt;/p&gt;
&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Conversion of DICOM series to DEN format&lt;/h3&gt;
&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;For the purpose of this example, we have to transform the DICOM dataset into the &lt;a href="https://kulvait.github.io/KCT_doc/den-format.html"&gt;DEN format&lt;/a&gt;, which is used by KCT framework. We do it using &lt;a href="https://github.com/kulvait/KCT_denpy"&gt;KCT_denpy package&lt;/a&gt;. This package allow us to convert between the representations of the volume data as numpy.ndarray, DEN file or DICOM series. Internally it uses the package pydicom to read DICOM data.&lt;/p&gt;
&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;For the conversion, I have created a simple script &lt;code&gt;[DICOMTODEN.py](https://github.com/kulvait/KCT_scripts/blob/master/DICOMTODEN.py)&lt;/code&gt;. The command to convert the data to the DEN format will then be&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; DICOM DEN_HU
&lt;/pre&gt;
&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;After you run this command, the new directory &lt;code&gt;DEN_HU&lt;/code&gt; will be created, which contains file &lt;code&gt;Series_00.den&lt;/code&gt;. This is a converted DICOM file into the DEN format. To vizualize the volume can be used &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/KCT_Den_File_Opener-1.1.1.jar"&gt;KCT Den file opener&lt;/a&gt;, a plugin of &lt;a href="https://imagej.nih.gov/ij/"&gt;ImageJ&lt;/a&gt; viewer that works also with &lt;a href="https://imagej.net/software/fiji/"&gt;Fiji&lt;/a&gt;. It is sufficient to put a file KCT_Den_File_Opener-1.1.1.jar into the .imagej/plugins configuration directory of ImageJ. Then simpy select &lt;code&gt;File-&amp;gt;Open DEN ..&lt;/code&gt; and navigate to &lt;code&gt;Series_00.den&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;You can notice that the values are integers starting from $-1024$ in the areas of the air up to values greater than $1000$ in the dense bone areas. These are in (Hounsfield units)[https://en.wikipedia.org/wiki/Hounsfield_scale], which are good for the physicans to compare tissue contrasts in a convenient scale, but they do not represent raw attenuation values. To do a projection of the data using KCT or reconstruction of these projections, we need raw attenuation values.&lt;/p&gt;
&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Conversion between Hounsfield units and raw attenuation values is possible using (KCT dentk toolkit)[https://github.com/kulvait/KCT_dentk]. Install the tool &lt;code&gt;dentk-fromhu&lt;/code&gt; to your path and then the conversion can be done either by calling&lt;/p&gt;
&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;dentk-fromhu Series_00.den Series_00.raw
&lt;/pre&gt;
&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;or using the script &lt;code&gt;DICOMTODEN.py&lt;/code&gt; to internally call &lt;code&gt;dentk-fromhu&lt;/code&gt; and to produce raw attenuations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;pre class="code literal-block"&gt;DICOMTODEN.py --force --dicom-file-suffix &lt;span class="s2"&gt;"dcm"&lt;/span&gt; --convert-hu-to-raw --base2 DICOM DEN_RAW
&lt;/pre&gt;
&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;There are two caveats. First for the conversion is important the attenuation value of the water, which in Hounsfield units is exactly $0$. By default the conversion will be performed in the same way as with the setting &lt;code&gt;--water-value 0.027&lt;/code&gt;. If you know the exact attenuation value of the water for given scanner, feel free to change it.&lt;/p&gt;
&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Second there is a problem that in &lt;a href="https://en.wikipedia.org/wiki/Hounsfield_scale"&gt;wiki article about Hounsfield units&lt;/a&gt; as well as in virtually every CT textbook, you can find that the minimum Hounsfield unit is 1000. DICOM data from SIEMENS scanners tend to have this minimum value $-1024$. It is due to the better data alignment with uint16 or uint32 representation of offsetted Hounsfield units. Standard conversion formula would however produce negative attenuation values. To correct for this undesired behavior we have to add &lt;code&gt;--base2&lt;/code&gt; as a parameter either to &lt;code&gt;DICOMTODEN.py&lt;/code&gt; or &lt;code&gt;dentk-fromhu&lt;/code&gt; program.&lt;/p&gt;
&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Vizualizing DEN volume of raw attenuation values derived from DICOM series&lt;/h3&gt;
&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;When you did everything correct, in the directory &lt;code&gt;DEN_RAW&lt;/code&gt; is the file &lt;code&gt;Series_00.den&lt;/code&gt;, which represent raw attenuation values derived from given DICOM series. You can also download &lt;a href="https://github.com/kulvait/KCT_den_file_opener/releases/download/v1.1.1/ExampleVolumeKCT_TCGA-19-1787.tar.xf"&gt;Example Volume TCGA-19-1787&lt;/a&gt;, which contain also the converted DEN volume.&lt;/p&gt;
&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;When vizualizing in ImageJ, you can select an area of the soft tissue and press ALT+CTRL+C. Then you can adjust the contrast. &lt;/p&gt;
&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;Investigating the volume in more detail from the perspective of data alignment it is apparent that the convention described on the picture &lt;a href="https://www.slicer.org/wiki/File:Coordinate_sytems.png"&gt;CT coordinates, image from the project slicer&lt;/a&gt; is observed. So that z axis is the axis of the rotation. Y axis goes from the top (above scanned subject) to bottom (under scanned object). Note, that ImageJ places the (0,0) top left and the positive direction of the Y axis is also top to bottom so that the axial image is vizualized in a natural orientation.  &lt;/p&gt;
&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;h3&gt;Working with KCT CBCT 2&lt;/h3&gt;
&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;

&lt;p&gt;In the next post, I will explain what are the conventions for the projection geometry and how to describe the geometric setting of the flat detector CT. We define an example setup of a virtual flat detector CT machine when scannin the volume created in this post. We then use KCT framework to project the volume and create projections.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;
</description><category>using_kct_blog</category><guid>https://kulvait.github.io/KCT_doc/posts/working-with-kct-cbct-1-converting-dicom-volume-to-den-volume-using-dicom-data-from-public-repository.html</guid><pubDate>Mon, 13 Sep 2021 19:05:35 GMT</pubDate></item></channel></rss>